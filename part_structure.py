import os
import random
import shutil
from tqdm import tqdm

# -------------------------- æ ¸å¿ƒé…ç½®å‚æ•° --------------------------
# æ•°æ®é›†æ ¹è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
dataset_root = "/Users/alin/Graduation_Project/VOCdevkit/VOC2026"
# åŸå§‹å›¾åƒ/æ ‡æ³¨ç›®å½•ï¼ˆä¸Šä¼ çš„æ‰€æœ‰æ–‡ä»¶éƒ½æ”¾åœ¨è¿™é‡Œï¼‰
src_images_dir = os.path.join(dataset_root, "images")  # æ‰€æœ‰ä¸Šä¼ çš„å›¾åƒ
src_labels_dir = os.path.join(dataset_root, "labels")  # æ‰€æœ‰ä¸Šä¼ çš„TXTæ ‡æ³¨
# åˆ’åˆ†åè¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆtrain/valå­ç›®å½•ï¼‰
dst_images_dir = os.path.join(dataset_root, "images")  # å¤ç”¨åŸç›®å½•ï¼Œä»…ç”Ÿæˆåˆ’åˆ†ç´¢å¼•
dst_labels_dir = os.path.join(dataset_root, "labels")

# æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
train_percent = 0.9  # è®­ç»ƒé›†å æ¯”ï¼ˆéªŒè¯é›†=1-0.9=0.1ï¼‰
random_seed = 0  # éšæœºç§å­ï¼ˆä¿è¯åˆ’åˆ†ç»“æœå¯å¤ç°ï¼‰
SUPPORTED_IMG_FORMATS = ('.jpg', '.jpeg', '.png', '.bmp', '.JPG', '.PNG')  # æ”¯æŒçš„å›¾åƒæ ¼å¼

# -------------------------- åˆå§‹åŒ–ç›®å½• --------------------------
# åˆ›å»ºåˆ’åˆ†åçš„labels/trainã€labels/valç›®å½•
os.makedirs(os.path.join(dst_labels_dir, "train"), exist_ok=True)
os.makedirs(os.path.join(dst_labels_dir, "val"), exist_ok=True)
# åˆ›å»ºåˆ’åˆ†åçš„images/trainã€images/valç›®å½•ï¼ˆå¯é€‰ï¼šå¦‚éœ€ç‰©ç†åˆ†éš”å›¾åƒï¼Œå–æ¶ˆæ³¨é‡Šï¼‰
# os.makedirs(os.path.join(dst_images_dir, "train"), exist_ok=True)
# os.makedirs(os.path.join(dst_images_dir, "val"), exist_ok=True)

# ç»Ÿè®¡å˜é‡
total_valid_pairs = 0  # æœ‰æ•ˆå›¾åƒ+æ ‡æ³¨å¯¹æ•°é‡
train_count = 0  # è®­ç»ƒé›†æ•°é‡
val_count = 0  # éªŒè¯é›†æ•°é‡
missing_labels = []  # ç¼ºå°‘æ ‡æ³¨çš„å›¾åƒ
missing_images = []  # ç¼ºå°‘å›¾åƒçš„æ ‡æ³¨


# -------------------------- ç¬¬ä¸€æ­¥ï¼šç­›é€‰æœ‰æ•ˆæ•°æ®å¹¶åˆ’åˆ† --------------------------
def get_valid_data_pairs():
    """
    éå†å›¾åƒå’Œæ ‡æ³¨ç›®å½•ï¼Œç­›é€‰å‡º å›¾åƒ-TXTæ ‡æ³¨ ä¸€ä¸€å¯¹åº”çš„æœ‰æ•ˆæ•°æ®å¯¹
    è¿”å›ï¼šæ‰€æœ‰æœ‰æ•ˆæ•°æ®çš„IDï¼ˆæ–‡ä»¶åï¼Œä¸å«åç¼€ï¼‰
    """
    # 1. è·å–æ‰€æœ‰å›¾åƒIDï¼ˆä¸å«åç¼€ï¼‰
    image_ids = set()
    for img_file in os.listdir(src_images_dir):
        img_name, img_ext = os.path.splitext(img_file)
        if img_ext.lower() in SUPPORTED_IMG_FORMATS:
            image_ids.add(img_name)

    # 2. è·å–æ‰€æœ‰æ ‡æ³¨IDï¼ˆä¸å«åç¼€ï¼‰
    label_ids = set()
    for label_file in os.listdir(src_labels_dir):
        label_name, label_ext = os.path.splitext(label_file)
        if label_ext.lower() == '.txt':
            label_ids.add(label_name)

    # 3. ç­›é€‰æœ‰æ•ˆæ•°æ®å¯¹ï¼ˆå›¾åƒå’Œæ ‡æ³¨éƒ½å­˜åœ¨ï¼‰
    valid_ids = list(image_ids & label_ids)  # äº¤é›†
    total_valid = len(valid_ids)

    # 4. ç»Ÿè®¡ç¼ºå¤±æƒ…å†µ
    global missing_labels, missing_images
    missing_labels = list(image_ids - label_ids)  # æœ‰å›¾åƒä½†æ— æ ‡æ³¨
    missing_images = list(label_ids - image_ids)  # æœ‰æ ‡æ³¨ä½†æ— å›¾åƒ

    # è¾“å‡ºæ•°æ®æ ¡éªŒç»“æœ
    print("ğŸ“Š æ•°æ®é›†æ ¡éªŒç»“æœï¼š")
    print(f"   æ€»å›¾åƒæ•°ï¼š{len(image_ids)}")
    print(f"   æ€»æ ‡æ³¨æ•°ï¼š{len(label_ids)}")
    print(f"   æœ‰æ•ˆæ•°æ®å¯¹ï¼ˆå›¾åƒ+æ ‡æ³¨ï¼‰ï¼š{total_valid}")
    if missing_labels:
        print(f"   âš ï¸  ç¼ºå°‘æ ‡æ³¨çš„å›¾åƒï¼š{len(missing_labels)} ä¸ªï¼ˆç¤ºä¾‹ï¼š{missing_labels[:5]}...ï¼‰")
    if missing_images:
        print(f"   âš ï¸  ç¼ºå°‘å›¾åƒçš„æ ‡æ³¨ï¼š{len(missing_images)} ä¸ªï¼ˆç¤ºä¾‹ï¼š{missing_images[:5]}...ï¼‰")

    if total_valid == 0:
        raise ValueError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå›¾åƒ+æ ‡æ³¨å¯¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå‘½åï¼")

    return valid_ids


def split_train_val(valid_ids):
    """
    æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†
    valid_ids: æœ‰æ•ˆæ•°æ®IDåˆ—è¡¨
    è¿”å›ï¼štrain_ids, val_ids
    """
    random.seed(random_seed)  # å›ºå®šç§å­ï¼Œåˆ’åˆ†ç»“æœå¯å¤ç°
    num_train = int(len(valid_ids) * train_percent)
    train_ids = random.sample(valid_ids, num_train)
    val_ids = [id for id in valid_ids if id not in train_ids]

    print(f"\nğŸ“¤ æ•°æ®é›†åˆ’åˆ†ç»“æœï¼š")
    print(f"   è®­ç»ƒé›†ï¼š{len(train_ids)} ä¸ª")
    print(f"   éªŒè¯é›†ï¼š{len(val_ids)} ä¸ª")
    return train_ids, val_ids


# -------------------------- ç¬¬äºŒæ­¥ï¼šåŒæ­¥åˆ’åˆ†å›¾åƒå’Œæ ‡æ³¨ --------------------------
def copy_files_to_split_dirs(data_ids, split_type):
    """
    å°†æŒ‡å®šIDçš„å›¾åƒå’Œæ ‡æ³¨å¤åˆ¶/ç§»åŠ¨åˆ°å¯¹åº”splitç›®å½•ï¼ˆtrain/valï¼‰
    data_ids: è¦å¤„ç†çš„æ•°æ®IDåˆ—è¡¨
    split_type: "train" æˆ– "val"
    """
    global train_count, val_count
    count = 0

    # ç›®æ ‡ç›®å½•
    dst_label_dir = os.path.join(dst_labels_dir, split_type)
    # å¦‚éœ€ç‰©ç†åˆ†éš”å›¾åƒï¼Œå–æ¶ˆä»¥ä¸‹æ³¨é‡Š
    # dst_img_dir = os.path.join(dst_images_dir, split_type)
    # os.makedirs(dst_img_dir, exist_ok=True)

    for data_id in tqdm(data_ids, desc=f"å¤„ç†{split_type}é›†"):
        # 1. å¤„ç†æ ‡æ³¨æ–‡ä»¶ï¼ˆå¤åˆ¶åˆ°splitç›®å½•ï¼‰
        src_label = os.path.join(src_labels_dir, f"{data_id}.txt")
        dst_label = os.path.join(dst_label_dir, f"{data_id}.txt")
        if os.path.exists(src_label):
            shutil.copy2(src_label, dst_label)  # å¤åˆ¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰ï¼Œå¦‚éœ€ç§»åŠ¨ç”¨shutil.move

        # 2. å¤„ç†å›¾åƒæ–‡ä»¶ï¼ˆå¯é€‰ï¼šç‰©ç†åˆ†éš”å›¾åƒï¼Œå–æ¶ˆä»¥ä¸‹æ³¨é‡Šï¼‰
        # æŸ¥æ‰¾å¯¹åº”å›¾åƒæ–‡ä»¶ï¼ˆåŒ¹é…æ‰€æœ‰æ”¯æŒçš„æ ¼å¼ï¼‰
        # src_img = None
        # for ext in SUPPORTED_IMG_FORMATS:
        #     temp_img = os.path.join(src_images_dir, f"{data_id}{ext}")
        #     if os.path.exists(temp_img):
        #         src_img = temp_img
        #         break
        # if src_img:
        #     dst_img = os.path.join(dst_img_dir, os.path.basename(src_img))
        #     shutil.copy2(src_img, dst_img)

        count += 1

    # æ›´æ–°ç»Ÿè®¡
    if split_type == "train":
        train_count = count
    else:
        val_count = count
    print(f"âœ… {split_type}é›†å¤„ç†å®Œæˆï¼š{count} ä¸ªæ•°æ®å¯¹")


# -------------------------- ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆYOLOè®­ç»ƒç´¢å¼•æ–‡ä»¶ --------------------------
def generate_yolo_index_files(train_ids, val_ids):
    """
    ç”Ÿæˆyolo_train.txtå’Œyolo_val.txtï¼ˆåŒ…å«å›¾åƒçš„ç»å¯¹è·¯å¾„ï¼‰ï¼Œç”¨äºYOLOè®­ç»ƒ
    """
    # ç”Ÿæˆè®­ç»ƒé›†ç´¢å¼•
    with open(os.path.join(dataset_root, "yolo_train.txt"), "w", encoding="utf-8") as f:
        for data_id in train_ids:
            # æŸ¥æ‰¾å›¾åƒç»å¯¹è·¯å¾„
            for ext in SUPPORTED_IMG_FORMATS:
                img_path = os.path.join(src_images_dir, f"{data_id}{ext}")
                if os.path.exists(img_path):
                    f.write(os.path.abspath(img_path) + "\n")
                    break

    # ç”ŸæˆéªŒè¯é›†ç´¢å¼•
    with open(os.path.join(dataset_root, "yolo_val.txt"), "w", encoding="utf-8") as f:
        for data_id in val_ids:
            # æŸ¥æ‰¾å›¾åƒç»å¯¹è·¯å¾„
            for ext in SUPPORTED_IMG_FORMATS:
                img_path = os.path.join(src_images_dir, f"{data_id}{ext}")
                if os.path.exists(img_path):
                    f.write(os.path.abspath(img_path) + "\n")
                    break

    print(f"\nğŸ“œ YOLOè®­ç»ƒç´¢å¼•æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ç´¢å¼•ï¼š{os.path.join(dataset_root, 'yolo_train.txt')}")
    print(f"   - éªŒè¯é›†ç´¢å¼•ï¼š{os.path.join(dataset_root, 'yolo_val.txt')}")


# -------------------------- ä¸»å‡½æ•°å…¥å£ --------------------------
if __name__ == "__main__":
    try:
        # 1. ç­›é€‰æœ‰æ•ˆæ•°æ®å¯¹
        valid_ids = get_valid_data_pairs()

        # 2. åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†
        train_ids, val_ids = split_train_val(valid_ids)

        # 3. åŒæ­¥åˆ’åˆ†æ ‡æ³¨æ–‡ä»¶ï¼ˆå›¾åƒå¯é€‰ç‰©ç†åˆ†éš”ï¼‰
        copy_files_to_split_dirs(train_ids, "train")
        copy_files_to_split_dirs(val_ids, "val")

        # 4. ç”ŸæˆYOLOè®­ç»ƒæ‰€éœ€çš„ç´¢å¼•æ–‡ä»¶
        generate_yolo_index_files(train_ids, val_ids)

        # æœ€ç»ˆç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ‰ æ•°æ®é›†åˆ’åˆ†å…¨éƒ¨å®Œæˆï¼æœ€ç»ˆç»Ÿè®¡ï¼š")
        print(f"   æ€»æœ‰æ•ˆæ•°æ®å¯¹ï¼š{len(valid_ids)}")
        print(f"   è®­ç»ƒé›†ï¼š{train_count} ä¸ªï¼ˆæ ‡æ³¨å·²å¤åˆ¶åˆ° {os.path.join(dst_labels_dir, 'train')}ï¼‰")
        print(f"   éªŒè¯é›†ï¼š{val_count} ä¸ªï¼ˆæ ‡æ³¨å·²å¤åˆ¶åˆ° {os.path.join(dst_labels_dir, 'val')}ï¼‰")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        exit(1)